

在 k8s 环境里，prometheus 的服务发现逻辑比较粗糙，endpoint 的变化导致的 reload 会使 prometheus 产生较大的负载波动。
查询冷数据(一个月之前的数据)或不合理 promQL 导致的慢查询所致的 prometheus 负载过高问题


### 时序数据库的关键技术
1）缓存：通过缓存来做访问加速，可以有多级缓存。
2）顺序写盘 + 合并写入：提高磁盘吞吐。
3）SSD：提高 IO 性能。
4）索引：提高读性能，可以有多级索引、倒排索引。
5）mmap：使用操作系统自己的内存管理。
6）有序化处理：方便做交集、查找、索引。
7）压缩：降低磁盘空间开销。
8）备份：提高可用性。
9）sharding：提高伸缩性。
10）WAL：write ahead log，辅助内存数据的可靠落地、延迟落地。 -- 预写日志

#### 高基数的难题
由于 prometheus 的数据模型是按 metric 名 + 多维度值做 key 来表示唯一的数据序列，
因此维度本身的取值就和数据序列的数量紧密关联，试想将用户标识 uid 这种取值范围巨大的变量作为一个维度，那么数据序列数必然就炸掉了

这种不好解决，只能想办法规避

如果要解决的话，考虑从存储和查询层面看
```text
存储层面：本身就好存放这么多数据，只能考虑压缩 -- 减少存储成本
查询层面：数据量大，查询缓慢，考虑做索引(多级索引)，但是索引本身也会额外占用磁盘，数据量超级大的情况，索引也会很大（额外的存储成本）
```


规避方式：
```text
关于label值的不够收敛导致的高基数问题，确实没有太好的办法，目前我们的思路是：
1、跟组内宣讲同步这个高基数的问题，让团队有意识的定义合理的label
2、目前大家的代码提交是经过MR流程，可以通过review规避些问题 
3、我们是Pull模式，exporter我们是自己基于Prometheus/client_golang 自己实现的，后面会对在exporter这一层对高基数问题进行监控、写错误日志并过滤  -- 最好在指标上报出过滤，如果写到了prometheus中，也会对prometheus造成性能影响
```

```text
时序数据分析是属于一类特殊的实时数据分析（OLAP）场景，常见的OLAP数据库Elasticsearch、ClickHouse、Druid等在处理高基数数据时是有优势的，
而像Prometheus、InfluxDB这类时序数据库，由于针对时序数据的特性做了针对性的性能优化，从而一定程度上牺牲了处理高基数数据的能力。
在数据量不大的场景下，也可以考虑采用通用的OLAP数据库来替代专用的时序数据库。

对于高基数的数据分析场景，常见的方案是把数据以log/trace的形式上报，然后根据实际的使用场景来决定后续的处理方式：
1. 实时，灵活数据查询，查询请求量小：通用的OLAP数据库
2. 实时，查询规则固定，查询请求量大：由Storm、Flink等流式计算引擎计算后落存储进行查询
3. 非实时，查询规则固定：由Spark、MapReduce等离线计算引擎计算后落存储进行查询
```


### prometheus的踩坑经验

高基数的指标(label的值成千上万)，导致在查询的卡死---prometheus的cpu使用量飙升

标采集参数配置了 honor_timestamps 为 false，导致采集到的 cpu 曲线不准确
```text
这里的根本原因是，prometheus采集exporter的指标数据时，不是使用的指标采集时间，而是使用抓取指标的时间，导致了曲线有问题
```