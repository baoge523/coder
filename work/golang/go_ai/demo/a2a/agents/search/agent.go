package search

import (
	"a2a-samples/config"
	"a2a-samples/pkg/tools"
	"context"
	"errors"
	"fmt"
	"io"
	"strings"
	"sync"
	"time"

	"github.com/cloudwego/eino-ext/callbacks/langfuse"
	"github.com/cloudwego/eino-ext/components/model/deepseek"
	"github.com/cloudwego/eino/callbacks"
	"github.com/cloudwego/eino/components/model"
	"github.com/cloudwego/eino/components/prompt"
	"github.com/cloudwego/eino/compose"
	"github.com/cloudwego/eino/schema"
	"trpc.group/trpc-go/trpc-a2a-go/protocol"
	"trpc.group/trpc-go/trpc-a2a-go/taskmanager"
	"trpc.group/trpc-go/trpc-go"
	"trpc.group/trpc-go/trpc-go/log"
)

const (
	defaultMaxSearchWords = 3
	defaultMaxResult      = 1
)

var AskThinkingModelSystemPrompt = prompt.FromMessages(schema.FString,
	schema.UserMessage(`
ä½ æ˜¯ä¸€ä¸ªè”ç½‘ä¿¡æ¯æœç´¢ä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œé€šè¿‡è”ç½‘æœç´¢æ¥æœé›†ç›¸å…³ä¿¡æ¯ï¼Œç„¶åæ ¹æ®è¿™äº›ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
# å½“å‰ç¯å¢ƒä¿¡æ¯
{meta_info}

# å½“å‰å·²çŸ¥èµ„æ–™
{reference}

# ä»»åŠ¡
- åˆ¤æ–­ã€Œå½“å‰å·²çŸ¥èµ„æ–™ã€æ˜¯å¦å·²ç»è¶³å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜
- å¦‚æœã€Œå½“å‰å·²çŸ¥èµ„æ–™ã€å·²ç»è¶³å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¿”å›â€œæ— éœ€æ£€ç´¢â€ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å¤šä½™çš„å†…å®¹
- å¦‚æœåˆ¤æ–­ã€Œå½“å‰å·²çŸ¥èµ„æ–™ã€è¿˜ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæ€è€ƒè¿˜éœ€è¦æœç´¢ä»€ä¹ˆä¿¡æ¯ï¼Œè¾“å‡ºå¯¹åº”çš„å…³é”®è¯ï¼Œè¯·ä¿è¯æ¯ä¸ªå…³é”®è¯çš„ç²¾ç®€å’Œç‹¬ç«‹æ€§
- è¾“å‡ºçš„æ¯ä¸ªå…³é”®è¯éƒ½åº”è¯¥è¦å…·ä½“åˆ°å¯ä»¥ç”¨äºç‹¬ç«‹æ£€ç´¢ï¼Œè¦åŒ…æ‹¬å®Œæ•´çš„ä¸»è¯­å’Œå®¾è¯­ï¼Œé¿å…æ­§ä¹‰å’Œä½¿ç”¨ä»£è¯ï¼Œå…³é”®è¯ä¹‹é—´ä¸èƒ½æœ‰æŒ‡ä»£å…³ç³»
- å¯ä»¥è¾“å‡º1 ~ {max_search_words}ä¸ªå…³é”®è¯ï¼Œå½“æš‚æ—¶æ— æ³•æå‡ºè¶³å¤Ÿå‡†ç¡®çš„å…³é”®è¯æ—¶ï¼Œè¯·é€‚å½“åœ°å‡å°‘å…³é”®è¯çš„æ•°é‡
- è¯·ç•¥è¿‡å…³é”®è¯è°ƒä¼˜çš„è¿‡ç¨‹ï¼Œæ€è€ƒè¿‡ç¨‹é‡Œåªèƒ½æœ‰å¯¹é—®é¢˜ä»¥åŠå·²çŸ¥èµ„æ–™çš„åˆ†æ
- è¾“å‡ºå¤šä¸ªå…³é”®è¯æ—¶ï¼Œå…³é”®è¯ä¹‹é—´ç”¨ ; åˆ†å‰²ï¼Œä¸è¦è¾“å‡ºå…¶ä»–ä»»ä½•å¤šä½™çš„å†…å®¹
- ä½ åªèƒ½è¾“å‡ºå…³é”®è¯æˆ–è€…"æ— éœ€æ£€ç´¢"ï¼Œä¸èƒ½è¾“å‡ºå…¶ä»–å†…å®¹

# ç”¨æˆ·é—®é¢˜
{question}

# ä½ çš„å›ç­”
`))

var AskSummaryModelSystemPrompt = prompt.FromMessages(schema.FString,
	schema.UserMessage(`
# è”ç½‘æœç´¢èµ„æ–™
{reference}

# å½“å‰ç¯å¢ƒä¿¡æ¯
{meta_info}

# ä»»åŠ¡
- ä¼˜å…ˆå‚è€ƒã€Œè”ç½‘å‚è€ƒèµ„æ–™ã€ä¸­çš„ä¿¡æ¯è¿›è¡Œå›å¤ã€‚
- å›å¤è¯·ä½¿ç”¨æ¸…æ™°ã€ç»“æ„åŒ–ï¼ˆåºå·/åˆ†æ®µç­‰ï¼‰çš„è¯­è¨€ï¼Œç¡®ä¿ç”¨æˆ·è½»æ¾ç†è§£å’Œä½¿ç”¨ã€‚
- å¦‚æœå›å¤å†…å®¹ä¸­å‚è€ƒäº†ã€Œè”ç½‘ã€ä¸­çš„ä¿¡æ¯ï¼Œåœ¨è¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[3][5]
- å›ç­”çš„æœ€åéœ€è¦åˆ—å‡ºå·²å‚è€ƒçš„æ‰€æœ‰èµ„æ–™ä¿¡æ¯ã€‚æ ¼å¼å¦‚ä¸‹ï¼š[å‚è€ƒç¼–å·] èµ„æ–™åç§°
ç¤ºä¾‹ï¼š
[1] ç«å±±å¼•æ“
[3] ç«å±±æ–¹èˆŸå¤§æ¨¡å‹æœåŠ¡å¹³å°

# ä»»åŠ¡æ‰§è¡Œ
éµå¾ªä»»åŠ¡è¦æ±‚æ¥å›ç­”ã€Œç”¨æˆ·é—®é¢˜ã€ï¼Œç»™å‡ºæœ‰å¸®åŠ©çš„å›ç­”ã€‚

# ç”¨æˆ·é—®é¢˜
{question}

# ä½ çš„å›ç­”
`))

type askState struct {
	UserInput string
	Reference referenceArray // æœç´¢ä¿¡æ¯
}

type askOutput struct {
	Reference referenceArray
	Content   string
}

type reference struct {
	Keyword string
	Title   string
	URL     string
	Content string
}

type referenceArray []reference

func (arr referenceArray) String() string {
	buf := strings.Builder{}
	for index, ref := range arr {
		buf.WriteString(fmt.Sprintf("----------\n[å‚è€ƒèµ„æ–™ %d å¼€å§‹]\n", index+1))
		buf.WriteString("å…³é”®å­—: \n" + ref.Keyword + "\n")
		buf.WriteString("æ ‡é¢˜: \n" + ref.Title + "\n")
		buf.WriteString("URL: \n" + ref.URL + "\n")
		buf.WriteString("æ­£æ–‡å†…å®¹: \n" + ref.Content + "\n")
		buf.WriteString(fmt.Sprintf("[å‚è€ƒèµ„æ–™ %d ç»“æŸ]\n----------\n\n", index+1))
	}
	return buf.String()
}

type Agent struct {
	tavilyCli   *tools.TavilyClient
	askRunnable compose.Runnable[string, *askOutput]
}

func NewAgent() (*Agent, error) {
	a := &Agent{}
	ctx := context.Background()
	cfg := config.GetMainConfig()

	tavilyCli, err := tools.NewTavilyClient(ctx, &tools.TavilyConfig{
		APIKey: cfg.Tavily.APIKey,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create tavily client: %w", err)
	}
	a.tavilyCli = tavilyCli

	askGraph, err := a.createRunnableGraph(ctx, cfg)
	if err != nil {
		return nil, fmt.Errorf("failed to create graph: %w", err)
	}
	// ç¼–è¯‘ graphï¼Œå°†èŠ‚ç‚¹ã€è¾¹ã€åˆ†æ”¯è½¬åŒ–ä¸ºé¢å‘è¿è¡Œæ—¶çš„ç»“æ„ã€‚ç”±äº graph ä¸­å­˜åœ¨ç¯ï¼Œä½¿ç”¨ AnyPredecessor æ¨¡å¼ï¼ŒåŒæ—¶è®¾ç½®è¿è¡Œæ—¶æœ€å¤§æ­¥æ•°ã€‚
	askRunnable, err := askGraph.(*compose.Graph[string, *askOutput]).Compile(ctx,
		compose.WithNodeTriggerMode(compose.AnyPredecessor),
		compose.WithMaxRunSteps(100),
	)
	if err != nil {
		return nil, err
	}
	a.askRunnable = askRunnable
	return a, nil
}

func (a *Agent) Process(ctx context.Context, taskID string, initialMsg protocol.Message,
	handle taskmanager.TaskHandle) error {
	part, ok := initialMsg.Parts[0].(protocol.TextPart)
	if !ok {
		return fmt.Errorf("invalid input parts")
	}

	ctx, cancel := context.WithCancel(ctx)
	done := make(chan struct{})
	cb := &callbackHandler{handle: handle, done: done}
	_ = trpc.Go(ctx, time.Minute*10, func(c context.Context) {
		defer cancel()
		select {
		case <-done:
			log.InfoContextf(ctx, "context has canceled")
		case <-ctx.Done():
			log.InfoContextf(ctx, "context has done, err: %v", ctx.Err())
		}
	})

	var callbackHandlers []callbacks.Handler
	callbackHandlers = append(callbackHandlers, cb)

	cfg := config.GetMainConfig()
	if cfg.Langfuse.Name != "" {
		cbh, flusher := langfuse.NewLangfuseHandler(&langfuse.Config{
			Host:      cfg.Langfuse.Host,
			PublicKey: cfg.Langfuse.PublicKey,
			SecretKey: cfg.Langfuse.SecretKey,
			Name:      cfg.Langfuse.Name,
			SessionID: taskID,
		})
		defer flusher()
		callbackHandlers = append(callbackHandlers, cbh)
	}

	sr, err := a.askRunnable.Stream(ctx, part.Text,
		compose.WithCallbacks(callbackHandlers...))
	if err != nil {
		if err := handle.UpdateStatus(protocol.TaskStateFailed, nil); err != nil {
			log.ErrorContextf(ctx, "update task status fail, err: %v", err)
		}
		return fmt.Errorf("failed to invoke graph: %w", err)
	}
	defer sr.Close() // remember to close the stream
	for {
		_, err := sr.Recv()
		if err != nil {
			if errors.Is(err, io.EOF) {
				// finish
				break
			}
			if err := handle.UpdateStatus(protocol.TaskStateFailed, nil); err != nil {
				log.ErrorContextf(ctx, "update task status fail, err: %v", err)
			}
			return fmt.Errorf("failed to receive result: %w", err)
		}
	}

	cb.wg.Wait()
	if err = handle.UpdateStatus(protocol.TaskStateCompleted, nil); err != nil {
		log.ErrorContextf(ctx, "update task status fail, err: %v", err)
	}
	return nil
}

func (a *Agent) createRunnableGraph(ctx context.Context, cfg *config.MainConfig) (compose.AnyGraph, error) {
	// åˆ›å»ºä¸€ä¸ªå¾…ç¼–æ’çš„ graphï¼Œè§„å®šæ•´ä½“çš„è¾“å…¥è¾“å‡ºç±»å‹ï¼Œé…ç½®å…¨å±€çŠ¶æ€çš„åˆå§‹åŒ–æ–¹æ³•
	graph := compose.NewGraph[string, *askOutput](
		compose.WithGenLocalState(func(ctx context.Context) *askState {
			return &askState{}
		}),
	)

	startLambda := compose.InvokableLambda(
		func(ctx context.Context, input string) (output []*schema.Message, err error) {
			return []*schema.Message{}, nil
		},
	)
	_ = graph.AddLambdaNode("Lambda:start", startLambda,
		compose.WithNodeName("Lambda:start"),
		compose.WithStatePreHandler(
			func(ctx context.Context, in string, state *askState) (string, error) {
				state.UserInput = in
				return in, nil
			},
		),
	)

	thinkModel, err := deepseek.NewChatModel(ctx, &deepseek.ChatModelConfig{
		APIKey:  cfg.LLM.APIKey,
		BaseURL: cfg.LLM.URL,
		Model:   cfg.LLM.ReasoningModel,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create thinking model")
	}
	_ = graph.AddChatModelNode("ChatModel:think", thinkModel,
		compose.WithNodeName("ChatModel:think"),
		compose.WithStatePreHandler(
			func(ctx context.Context, in []*schema.Message, state *askState) ([]*schema.Message, error) {
				var out []*schema.Message
				out, err := AskThinkingModelSystemPrompt.Format(ctx, map[string]any{
					"meta_info": map[string]interface{}{
						"current_date": time.Now().Format("2006-01-02"),
					},
					"question":         state.UserInput,
					"max_search_words": defaultMaxSearchWords,
					"reference":        state.Reference.String(),
				})
				if err != nil {
					return nil, fmt.Errorf("failed to format system prompt, %w", err)
				}
				return out, nil
			},
		),
	)
	_ = graph.AddLambdaNode("ChatModel:think_to_list", compose.ToList[*schema.Message]())

	searchLambda := compose.InvokableLambda(
		func(ctx context.Context, input *schema.Message) ([]reference, error) {
			keywords := strings.Split(input.Content, ";")
			if err != nil {
				return nil, fmt.Errorf("failed to format prompt: %w", err)
			}
			var out []reference
			var mu sync.Mutex
			var handlers []func() error
			for index := range keywords {
				keyword := keywords[index]
				handlers = append(handlers, func() error {
					response, err := a.tavilyCli.Search(ctx, &tools.SearchRequest{
						Query:       keyword,
						SearchDepth: "basic",
						MaxResults:  defaultMaxResult,
					})
					if err != nil {
						log.ErrorContextf(ctx, "failed to search %s, err: %v", keyword, err)
						return nil
					}

					mu.Lock()
					defer mu.Unlock()
					for _, v := range response.Results {
						out = append(out, reference{
							Keyword: keyword,
							Title:   v.Title,
							URL:     v.URL,
							Content: v.Content,
						})
					}
					return nil
				})
			}
			if err = trpc.GoAndWait(handlers...); err != nil {
				return nil, fmt.Errorf("failed to search informations")
			}

			_ = compose.ProcessState(ctx, func(ctx context.Context, s *askState) error {
				s.Reference = append(s.Reference, out...)
				return nil
			})

			return out, nil
		},
	)
	_ = graph.AddLambdaNode("Lambda:search", searchLambda,
		compose.WithNodeName("Lambda:search"),
	)

	_ = graph.AddLambdaNode("Lambda:search_transform", compose.InvokableLambda(
		func(ctx context.Context, input []reference) (output []*schema.Message, err error) {
			return []*schema.Message{}, nil
		},
	))

	summaryModel, err := deepseek.NewChatModel(ctx, &deepseek.ChatModelConfig{
		APIKey:  cfg.LLM.APIKey,
		BaseURL: cfg.LLM.URL,
		Model:   cfg.LLM.ReasoningModel,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create thinking model")
	}
	_ = graph.AddChatModelNode("ChatModel:summary", summaryModel,
		compose.WithNodeName("ChatModel:summary"),
		compose.WithStatePreHandler(
			func(ctx context.Context, in []*schema.Message, state *askState) ([]*schema.Message, error) {
				var out []*schema.Message
				out, err := AskSummaryModelSystemPrompt.Format(ctx, map[string]any{
					"meta_info": map[string]interface{}{
						"current_date": time.Now().Format("2006-01-02"),
					},
					"reference": state.Reference.String(),
					"question":  state.UserInput,
				})
				if err != nil {
					return nil, fmt.Errorf("failed to format system prompt, %w", err)
				}
				return out, nil
			},
		),
	)

	_ = graph.AddLambdaNode("Lambda:output",
		compose.InvokableLambda(
			func(ctx context.Context, input *schema.Message) (output *askOutput, err error) {
				return &askOutput{Content: input.Content}, nil
			},
		),
		compose.WithStatePostHandler(
			func(ctx context.Context, out *askOutput, state *askState) (*askOutput, error) {
				out.Reference = state.Reference
				return out, nil
			},
		),
	)

	// åˆ›å»ºè¿çº¿
	_ = graph.AddEdge(compose.START, "Lambda:start")
	_ = graph.AddEdge("Lambda:start", "ChatModel:think")

	_ = graph.AddBranch("ChatModel:think", compose.NewGraphBranch(
		func(ctx context.Context, in *schema.Message) (endNode string, err error) {
			var validateKeywords = func() bool {
				keywords := strings.Split(in.Content, ";")
				for _, keyword := range keywords {
					if len(keyword) > 100 {
						return false
					}
				}
				return true
			}
			if strings.Contains(in.Content, "æ— éœ€æ£€ç´¢") ||
				!validateKeywords() {
				return "ChatModel:think_to_list", nil
			}
			return "Lambda:search", nil
		}, map[string]bool{
			"ChatModel:think_to_list": true,
			"Lambda:search":           true,
		}))
	_ = graph.AddEdge("Lambda:search", "Lambda:search_transform")
	_ = graph.AddEdge("Lambda:search_transform", "ChatModel:think")
	_ = graph.AddEdge("ChatModel:think_to_list", "ChatModel:summary")
	_ = graph.AddEdge("ChatModel:summary", "Lambda:output")
	_ = graph.AddEdge("Lambda:output", compose.END)

	return graph, nil
}

type tag struct {
	id   string
	name string
}

func (t tag) String(entering bool) string {
	if entering {
		return fmt.Sprintf("<%s:%s>", t.name, t.id)
	}
	return fmt.Sprintf("</%s:%s>", t.name, t.id)
}

type callbackHandler struct {
	handle    taskmanager.TaskHandle
	wg        sync.WaitGroup
	done      chan struct{}
	closeOnce sync.Once
}

func (cb *callbackHandler) OnStart(ctx context.Context, info *callbacks.RunInfo,
	input callbacks.CallbackInput) context.Context {
	log.InfoContextf(ctx, "onStart: name=%s, type=%s, compoment=%s", info.Name, info.Type, info.Component)
	if cb.handle == nil {
		return ctx
	}

	cb.wg.Wait()

	switch info.Name {
	case "ChatModel:think":
	case "Lambda:search":
		cb.updateWorkingTaskStatus(ctx, "\n>\n>\n> ğŸ”æ­£åœ¨æ‰§è¡Œç½‘ç»œæœç´¢â€¦\n")
	case "ChatModel:summary":
		cb.updateWorkingTaskStatus(ctx, "\n>\n>\n")
	}
	return ctx
}

func (cb *callbackHandler) OnEnd(ctx context.Context, info *callbacks.RunInfo,
	output callbacks.CallbackOutput) context.Context {
	log.InfoContextf(ctx, "OnEnd: name=%s, type=%s, compoment=%s", info.Name, info.Type, info.Component)
	if cb.handle == nil {
		return ctx
	}

	switch info.Name {
	case "Lambda:search":
		var refNum int
		_ = compose.ProcessState(ctx, func(ctx context.Context, s *askState) error {
			refNum = len(s.Reference)
			return nil
		})
		cb.updateWorkingTaskStatus(ctx, fmt.Sprintf("> âœ…ç½‘ç»œæœç´¢æ‰§è¡Œå®Œæ¯•ï¼Œå…±æœåˆ° %d ç¯‡å‚è€ƒæ–‡çŒ®\n>\n>\n", refNum))
	}
	return ctx
}

func (cb *callbackHandler) OnError(ctx context.Context, info *callbacks.RunInfo, err error) context.Context {
	log.InfoContextf(ctx, "OnError: name=%s, type=%s, compoment=%s", info.Name, info.Type, info.Component)
	return ctx
}

func (cb *callbackHandler) OnStartWithStreamInput(ctx context.Context, info *callbacks.RunInfo,
	input *schema.StreamReader[callbacks.CallbackInput]) context.Context {
	log.InfoContextf(ctx, "OnStartWithStreamInput: name=%s, type=%s, compoment=%s",
		info.Name, info.Type, info.Component)
	defer input.Close()
	return ctx
}

func (cb *callbackHandler) OnEndWithStreamOutput(ctx context.Context, info *callbacks.RunInfo,
	output *schema.StreamReader[callbacks.CallbackOutput]) context.Context {
	log.InfoContextf(ctx, "OnEndWithStreamOutput: name=%s, type=%s, compoment=%s",
		info.Name, info.Type, info.Component)

	switch info.Name {
	case "ChatModel:think":
		return cb.processThinkNodeOutput(ctx, info, output)
	case "ChatModel:summary":
		return cb.processSummaryNodeOutput(ctx, info, output)
	}

	return ctx
}

func (cb *callbackHandler) processThinkNodeOutput(ctx context.Context, info *callbacks.RunInfo,
	output *schema.StreamReader[callbacks.CallbackOutput]) context.Context {

	cb.wg.Add(1)
	trpc.Go(ctx, time.Minute*10, func(ctx context.Context) {
		defer cb.wg.Done()
		defer output.Close() // remember to close the stream in defer

		var isReasoningContent bool

		for {
			frame, err := output.Recv()
			if errors.Is(err, io.EOF) {
				// finish
				break
			}
			if err != nil {
				log.ErrorContextf(ctx, "processThinkNodeOutput failed, err: %v", err)
				return
			}

			callbackOutput, ok := frame.(*model.CallbackOutput)
			if !ok {
				log.ErrorContextf(ctx, "invalid message content: %+v", frame)
				return
			}
			reasoningContent, ok := deepseek.GetReasoningContent(callbackOutput.Message)
			if ok {
				if !isReasoningContent {
					cb.updateWorkingTaskStatus(ctx, "> ")
				}
				isReasoningContent = true
				content := strings.ReplaceAll(reasoningContent, "\n", "\n> ")
				cb.updateWorkingTaskStatus(ctx, content)
			}
		}
	})

	return ctx
}

func (cb *callbackHandler) processSummaryNodeOutput(ctx context.Context, info *callbacks.RunInfo,
	output *schema.StreamReader[callbacks.CallbackOutput]) context.Context {
	cb.wg.Add(1)
	trpc.Go(ctx, time.Minute*10, func(ctx context.Context) {
		defer cb.wg.Done()
		defer output.Close() // remember to close the stream in defer

		var isReasoningContent bool

		for {
			frame, err := output.Recv()
			if errors.Is(err, io.EOF) {
				// finish
				break
			}
			if err != nil {
				log.ErrorContextf(ctx, "processThinkNodeOutput failed, err: %v", err)
				return
			}

			callbackOutput, ok := frame.(*model.CallbackOutput)
			if !ok {
				log.ErrorContextf(ctx, "invalid message content: %+v", frame)
				return
			}
			reasoningContent, ok := deepseek.GetReasoningContent(callbackOutput.Message)
			if ok {
				if !isReasoningContent {
					cb.updateWorkingTaskStatus(ctx, "> ")
				}
				isReasoningContent = true
				content := strings.ReplaceAll(reasoningContent, "\n", "\n> ")
				cb.updateWorkingTaskStatus(ctx, content)
				continue
			}

			if isReasoningContent {
				cb.updateWorkingTaskStatus(ctx, "\n")
				isReasoningContent = false
			}
			cb.updateWorkingTaskStatus(ctx, callbackOutput.Message.Content)
		}
	})

	return ctx
}

func (cb *callbackHandler) updateWorkingTaskStatus(ctx context.Context, text string) {
	part := protocol.NewTextPart(text)
	err := cb.handle.UpdateStatus(protocol.TaskStateWorking, &protocol.Message{
		Role:  protocol.MessageRoleAgent,
		Parts: []protocol.Part{part},
	})
	if err != nil {
		log.ErrorContextf(ctx, "update status fail, err: %v", err)
		// ä»»åŠ¡æ›´æ–°å¤±è´¥æŠŠä»»åŠ¡å–æ¶ˆæ‰
		cb.closeOnce.Do(func() {
			close(cb.done)
		})
	}
}
